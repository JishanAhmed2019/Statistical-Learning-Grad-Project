---
title: "Project 2 - Supervised and Unsupervised learning methods"
output:
  html_document: default
  pdf_document: default
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# Overview of the data set 

We can implement supervised and unsupervised machine learning techniques in analysis of genomic data. In particular, principal components analysis (PCA), hierarchical clustering and random forest are very useful machine learning tools to analyze genomic data.  Illustration of these techniques has been presented in this project using the mRNA microarray data which is commonly known as Wang data.  Wang data consists of 52580 gene expression measurements on 22 samples. We have used count table and phenotype table in our study. From Phenotype table, we have extracted different cell type information to implement our random forest methods. We do not make use of the phenotype information in performing PCA and clustering, as these are unsupervised techniques. But after performing PCA and clustering, we will check to see the extent to which these phenotype agree with the results of these unsupervised techniques. In this project, we have attempted to use the results of PCA in order to implement supervised learning algorithm Random forest. 


First we read Count table and phenotype table to apply PCA, hierarchical clustering and random forest algorithms. 

### Data loading {.tabset}

```{r}
# Read data files from local disk
gene <- read.csv("C:/Users/User 1/Desktop/gene.csv")
pheno <- read.csv("C:/Users/User 1/Desktop/pheno.csv")
dim(gene)
length(pheno)
```

### Data preprocessing {.tabset}
We begin by examining the Phenotype.

```{r}
wang.pheno=pheno$cell.type
gene_Gene <- gene[,-1]

```

#Dimension reduction method 


##Principal Components Analysis (PCA)

We first perform PCA on the data without scaling the variables (genes), because it is better not to scale the genes. It is also required to transpose our matrix. 

```{r}
wang.data=t(gene_Gene)
```

```{r}
pr.out2 =prcomp (wang.data, scale=FALSE)
```


We now plot the first few principal component score vectors, in order to visualize the data. The observations corresponding to a given cell type will be plotted in the same color, so that we can see to what extent the observations within a cell type are similar to each other. We first create a simple function that assigns a distinct color to each element of a numeric vector. 


```{r}
Cols=function(vec){
  cols=rainbow (length(unique(vec )))
  return (cols[as.numeric(as.factor (vec))])
}

par(mfrow =c(1,2))
plot(pr.out2$x[,1:2], col =Cols(wang.pheno), pch =19, xlab ="Z1",ylab="Z2")
plot(pr.out2$x[,c(1,3)],col =Cols(wang.pheno), pch =19, xlab ="Z1",ylab="Z3")

```

We see that it would not have been possible to visualize the data without using a dimension reduction method such as PCA, since based on the full data set there are huge possible scatterplots, none of which would have been particularly informative. We can obtain a summary of the proportion of variance explained (PVE) of the first few principal components using the summary() method for a prcomp object.


```{r}
summary (pr.out2)
```

Using the plot() function, we can also plot the variance explained by the first few principal components.

```{r}
plot(pr.out2)
```

We observe that the height of each bar in the bar plot is given by squaring the corresponding element of pr.out$sdev. However, it is more informative to plot the PVE of each principal component which is known as a scree plot.

```{r}
pve = 100* pr.out2$sdev ^2/ sum(pr.out2$sdev ^2)
par(mfrow =c(1,2))
plot(pve , type ="o", ylab="PVE ", xlab="Principal Component", col ="blue")
plot(cumsum (pve ), type="o", ylab ="Cumulative PVE", xlab="Principal Component ", col ="brown3 ")
```

The elements of pve can also be computed directly as

```{r}
summary(pr.out2)$importance[2,]
summary(pr.out2)$importance[3,]
```

We see that together, the first 10 principal components explain around 98 %. However, looking at the scree plot, we see that while each of the first 10 principal components explain a substantial amount of variance, there is a marked decrease in the variance explained by further principal components. That is, there is an elbow in the plot after approximately the tenth principal component. This suggests that there may be little benefit to examining more than tenth or so principal components (though even examining ten principal components may be difficult).Together, all principal components explain 100% of the variance. 


#Unsupervised learning method

##Hierarchical clustering

We have implemented hierarchical clustering algorithm to hierarchically cluster the cell type in the Wang data, with the goal of finding out whether or not the observations cluster into distinct types of cell type. To begin, we standardize the variables to have mean zero and standard deviation one in order to to keep each gene on the same scale.


```{r}
# Standardize data to have mean 0 and stdev 1 
sd.data=scale(wang.data)
```


We now perform hierarchical clustering of the observations using complete, single, and average linkage. Euclidean distance has been used as a distance metric to measure dissimilarity.


```{r}
# Create distance matrix
data.dist=dist(sd.data)
```



```{r}
hc.single =hclust(data.dist, method ="single")
hc.complete =hclust(data.dist, method ="complete")
hc.average =hclust(data.dist, method ="average")
par(mfrow =c(3,1))
plot(hc.single, labels =wang.pheno, main="Single Linkage", xlab="",sub ="", ylab="" )
plot(hc.complete ,labels =wang.pheno, main ="Complete Linkage", xlab="", sub ="", ylab="" )
plot(hc.average , labels =wang.pheno, main ="Average Linkage", xlab="", sub ="", ylab="" )
```


We see that the choice of linkage certainly does affect the results obtained. Complete and average linkage tend to yield evenly sized clusters whereas single linkage tends to yield extended clusters to which single leaves are fused one by one. We will use complete linkage hierarchical clustering to analyze our findings. We can cut the dendrogram at the height that will yield a particular number of clusters. We have chosen three in this project.


```{r}
hc.out =hclust(dist(sd.data))
hc.clusters =cutree (hc.out ,3)
table(hc.clusters ,wang.pheno)
```


There are some clear patterns. All the cerebellum fall in same cluster. We can plot the cut on the dendrogram that produces three clusters:

```{r}
par(mfrow =c(1,1))
plot(hc.out , labels =wang.pheno)
abline (h=371, col ="red")
```

The abline() function draws a straight line on top of any existing plot in R . The argument h=371 plots a horizontal line at height 371 on the dendrogram; This is the height that results in three distinct clusters. It is easy to verify that the resulting clusters are the same as the ones we obtained using cutree(hc.out,3).

Printing the output of hclust gives a useful brief summary of the object:


```{r}
hc.out
```

#Supervised learning method

##Random forest
We have found ten dominant features in our PCA analysis. Here we have performed Random forest using reduced dimension of feature space. We have considered ten features from PCA to implement random forest. 
```{r}
dimension.data <- data.frame(wang.pheno, pr.out2$x)
pca.data <- dimension.data[,1:11]
dim(pca.data)
str(pca.data)

```

We have used randomForest() R package in this project. By default, it uses $\sqrt p $ variables when building a random forest of classification trees. We choose here default case that is $ m_{try} = sqrt(p) $. 
```{r}
set.seed(81)
library(MASS)
library(rpart)
library(randomForest)
```

# Modelling with Random forest

Cell type has been used here as a response variable. We can consider this classification as  multi class classification problems. We have attempted to classify 17 different cell type, namely BT474, HME , MB435, MCF7, T47D, adipose, brain, breast, cerebellum, colon, heart, liver, lymph node,, mixed brain, mixed human, skeletal muscle, testes. 

```{r}
rf.model <- randomForest(wang.pheno ~ ., data=pca.data, importance=TRUE)
```


```{r}
rf.model
#plot(rf.model)
```

We see that the number of variables has been tried at each split is 3. We have found OOB estimate of  error rate is 72.73%. 
We have also used our entire data as a training set to see our model performance.
```{r}
#colnames(pca.data)
#Prediction on training data
predicted.values <- predict(rf.model, pca.data[1:11])
d_pca <- table(predicted.values, pca.data$wang.pheno)
print(d_pca)
```

```{r}
library(caret)
confusionMatrix(predicted.values,pca.data$wang.pheno)
```

#Results 

In this project, we have used Wang data which is a high dimensional genomic data. From our PCA analysis, we have found that the first ten principal components are capable of explaining approximately 98% of the variance of the data. It is reasonable to use this ten components for our further investigation. We have then performed hierarchical clustering of the observations using complete, single, and average linkage along with Euclidean distance. We see that complete linkage performs better. It is noticeable that cerebellums were clustered in cluster three. Three distinct clusters are clearly visible in our denrogram. To explore more information regarding this data, we have implemented supervised learning algorithm Random forest. Random forest is really a good choice to classify this data. It has only 22 samples. This is too small to implement other available ML tools. We know that Random  forest utilizes bootstrap method. It has built in feature to perform cross validation. It gives us Out of Bag error which can be considered as test error rate. Hence we can skip cross validation. We have found estimate of OOB error rate is 72.73% which seems very high but this result is convincing. We have predicted pretty large number of classes i.e  17 classes from 22 samples using 10 predictors. To convince ourselves, we have made a prediction using our training data. We have observed that our model performed perfectly. Our classification accuracy, kappa accuracy, sensitivity, and specificity were 100%.  


# Conclusion 

In conclusion, it is always challenging to unfold the mystery of genomic data. Most of the ML tools are incapable of handling data with large predictors. In this project, we have successfully implemented dimension reduction method to explore the genomic data. Clustering methods also have helped us to visualize some patterns in our data. We have also explored the beauty of dimension reduction in implementing Random forest classification. It is hard to classify these kinds of genomic data with large number of predictors without dimension reduction. In this project, we have experienced that data preparation and transition from supervised to unsupervised learning are really a fascinating jobs. 
